"<p>Who's widely considered the founder of Computer Science &amp; Artificial Intelligence?</p>";"<p>Alan Turing (this handsome fella)</p><img src='aisffs-turing.jpg'>"
"<p>The two main eras of AI (more or less):</p>";"<p>Before 2000: Symbolic AI. After 2000: Deep Learning.</p>"
"<p>What was Symbolic AI good at, and bad at? What is Deep Learning currently good at, and bad at?</p>";"<p>Symbolic AI: good at Logic, bad at 'Intuition'. Deep Learning: good at 'Intuition', bad at Logic.</p>"
"<p>Another phrase for 'Symbolic AI':</p>";"<p>Good Old Fashioned AI (GOFAI)</p>"
"<p>When were the decades of Symbolic AI's dominance (roughly)?</p>";"<p>1950's to 1990's</p>"
"<p>In the Symbolic AI mindset, how do you make an AI?</p>";"<p>1) Write down the step-by-step rules to solve a problem. 2) Make a computer follow those steps <em>really fast</em>.</p>"
"<p>Why could Symbolic AI beat the world champion at chess, but not recognize pictures of cats?</p>";"<p>Because we don't consciously know the step-by-step rules we use to recognize cats (or other visual/'intuitive' tasks), hence can't code them into a Symbolic AI.</p>"
"<p>Logic: ▯▯▯▯-▯▯-▯▯▯▯. Intuition: ▯▯▯-▯▯-▯▯▯▯.</p>";"<p>Logic: step-by-step. Intuition: all-at-once.</p>"
"<p>Moravec's Paradox (paraphrased):</p>";"<p>What’s hard for humans is easy for AI; What’s easy for humans is hard for AI.</p>"
"<p>3 safety problems for AI Logic:</p>";"<p>1) No common sense, 2) 'Specification gaming', 3) 'Instrumental convergence'.</p>"
"<p>What is 'specification gaming'?</p>";"<p>When an AI achieves a goal in a literal, logical way... just not the way you wanted.</p>"
"<p>Why isn't a rule of 'follow common sense/expert consensus/the law' good enough for <em>humane</em> AI?</p>";"<p>It used to be common sense/expert consensus/the law that slavery is good &amp; natural. An AI told to follow common sense/experts/law would fight for unjust status-quos then, and now.</p>"
"<p>What is 'instrumental convergence'?</p>";"<p>The idea that almost all goals you can give to an AI would 'converge' on the same unsafe sub-goals ('instrumental' goals), such as resisting shutdown &amp; grabbing resources.</p>"
"<p>Why would a bot told to calculate digits of pi, have a logical incentive to <em>resist shutdown?</em></p>";"<p>Because you can't calculate digits of pi if you're shut down.</p>"
"<p>Why would a bot told to calculate digits of pi, have a logical incentive to <em>grab computation resources?</em></p>";"<p>Because you can calculate digits of pi <em>better</em> with more computation resources.</p>"
"<p>Instrumental Convergence does NOT depend on...</p>";"<p>...'super-human intelligence' or 'evolution-instilled desires to survive &amp; dominate'. (A common misconception!)</p>"
"<p>Around when was the artificial neuron invented?</p>";"<p>In the 1940's — (1943, to be exact) — <em>before</em> the phrase 'Artificial Intelligence' was coined!</p>"
"<p>Visualize what an Artificial Neural Network (ANN) looks like:</p>";"<img src='aisffs-ann.png'>"
"<p>Two synonyms for 'Artificial Neural Network':</p>";"<p>Perceptrons, Connectionist AI</p>"
"<p>Venn diagram of how AI, Symbolic AI, Machine Learning, and Deep Learning all relate to each other:</p>";"<img src='aisffs-venn.png'>"
"<p>Why was Connectionist AI buried for half a century?</p>";"<p>Because Symbolic AI dominated academia, and famous cognitive scientists dismissed artificial neural networks.</p>"
"<p>What decades did ANNs <em>really</em> make a comeback?</p>";"<p>2010's &amp; 20's. (You're living through it!)</p>"
"<p>3 safety problems with AI 'Intuition':</p>";"<p>1) It may learn human prejudices. 2) It breaks easily, and in weird ways. 3) It's an un-verifiable black box.</p>"
"<p>The name for the risk that AI learns human prejudices:</p>";"<p>Algorithmic Bias</p>"
"<p>A name for the risk of AI 'intuition' breaking easily, especially in slightly unusual situations:</p>";"<p>(Either answer works:) Out-of-Distribution Error, Robustness failure</p>"
"<p>A funny example of AI intuition breaking, in machine vision</p>";"<img src='aisffs-apple.jpg'>"
"<p>A name for the risk of AI's <em>goals</em> breaking, but its <em>skills</em> remain intact:</p>";"<p>(Either answer works:) Goal misgeneralization, Inner misalignment</p>"
"<p>Why is “broken goals, intact skills” more dangerous than an AI that just breaks entirely?</p>";"<p>Because then it can <em>skillfully</em> execute on the wrong goal!</p>"
"<p>Why don't we understand the insides of ANNs?</p>";"<p>Because ANNs are <em>not</em> hand-coded. They usually have millions or billions of parameters, found by 'trial-and-error'.</p>"
"<p>Another name for the 'modern AI is a black box' problem:</p>";"<p>The interpretability problem.</p>"
"<p>Moore's Law (<em>very</em> roughly paraphrased):</p>";"<p>“Every 2 years, computing power doubles.”</p>"
"<p>AI Scaling Law for GPT (<em>very</em> roughly paraphrased):</p>";"<p>“Every time you throw 1,000,000x more compute at training GPT, it gets 2x better.”</p>"
"<p>An argument that Moore's Law will end soon, if it hasn't already:</p>";"<p>We actually <em>can't</em> keep halving the size of transistors much further, before they become <em>smaller than single atoms</em>.</p>"
"<p>An argument that the end of Moore's Law and AI Scaling Laws may <em>not</em> mean a 3rd AI Winter:</p>";"<p>Like how ANNs and backpropagation were buried for decades, the next big idea could be hiding in plain sight, waiting to be (re-)discovered.</p>"
"<p>“System 1” is...</p>";"<p>Fast, all-at-once intuition. Thinking in vibes.</p>"
"<p>Examples of “System 1” thinking:</p>";"<p>(Any examples work, but here's what I wrote:) Recognizing pictures of cats, Balancing yourself on a bike.</p>"
"<p>“System 2” is...</p>";"<p>Slow, step-by-step logic. Thinking in gears.</p>"
"<p>Examples of “System 2” thinking:</p>";"<p>(Any examples work, but here's what I wrote:) Solving tricky math problems, Path-finding through an unfamiliar town.</p>"
"<p>The trajectories of Old AI vs New AI, on a “System 1 vs System 2” graph:</p>";"<img src='aisffs-trajectories.png'>"
"<p>Why can't we 'just combine' old &amp; new AI, to get AI that does both logic and intuition?</p>";"<p>Same reason we can't 'just combine' jets &amp; backpacks to get jetpacks: it's <em>very tricky</em> to combine things, sometimes.</p>"
"<p>The 2 awkward alliances in AI Safety:</p>";"<p>1) Between Capabilities 'versus' Safety, and 2) Between Near-Risk 'versus' Existential-Risk.</p>"
"<p>Why ‘Capabilities versus Safety’ is a useful-but-fake divide:</p>";"<p>Features can advance both capabilities <em>and</em> safety. (Analogy: brakes &amp; cruise control on cars)</p>"
"<p>The two axes of AI Risk concerns:</p>";"<p>1) Unintentional vs Intentional (or Accidents vs Abuse) 2) Bad vs Existential-Risk Bad</p>"
"<p>‘Artificial General Intelligence’ (AGI) is a vague way to point at:</p>";"<p>‘Software that can do important knowledge-based tasks at a human-expert level or better’. (e.g. Automatic scientific discovery)</p>"
"<p>When do AI experts predict we have a better-than-even chance of AGI?</p>";"<p>Median answer is 2060, but the uncertainty is <em>comically</em> huge. So: ¯\_(ツ)_/¯</p>"
"<p>‘AI Takeoff’ is the hypothetical scenario when...</p>";"<p>an AI can improve its ability to improve its ability to improve its ability to... (and so on)</p>"
"<p>The 3 types of predictions for how fast AI would self-improve in an AI Takeoff: (visualize)</p>";"<img src='aisffs-takeoffs.png'>"
"<p>Argument for a 'FOOM' takeoff:</p>";"<p>If at each step, an AI doubles its capability in half the time, the AI would reach infinity (or the theoretical maximum) in finite time.</p>"
"<p>Argument for an Exponential takeoff:</p>";"<p>All things that invest in themselves, like economies or pandemics, grow exponentially.</p>"
"<p>Argument for Steady or Decelerating Takeoff:</p>";"<p>All things, even if they grow exponentially at first, hit 'diminishing returns' and slow down.</p>"
"<p>Analogy for why a 'slow, steady' takeoff isn't necessarily safe:</p>";"<p>The Titanic was slow &amp; steady, yet still fatal.</p>"
"<p>The intuitive 'prior' for the Skeptic case on powerful AI:</p>";"<p>Humans have already survived for 300,000 years, chances that <em>this</em> is the (near-)final generation is low.</p>"
"<p>The intuitive 'prior' for the Cautious case on powerful AI:</p>";"<p>Usually in history, when a group meets a 'higher capabilities' group, it goes badly for the former.</p>"
"<p>Why trying to forecast 'probability of doom' may be a self-denying prophecy:</p>";"<p>If folks think P(doom) is low, they'll get complacent, so it'll happen. If folks think P(doom) is high, they'll react hard, and it'll be avoided</p>"
"<p>Visualization of good/bad AI trajectories, on a Safety vs Capabilities graph:</p>";"<img src='aisffs-goodbad.png'>"