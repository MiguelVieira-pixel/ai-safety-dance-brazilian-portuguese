"<p>Goal Mis-specification is:</p>";"<p>When an AI does <em>exactly what you asked for</em>, not necessarily <em>what you actually wanted.</em></p>"
"<p>Goodhart's Law, paraphrased:</p>";"<p>When you reward a metric, it usually gets gamed.</p>"
"<p>An example of Goodhart's Law applied to humans:</p>";"<p>(Any example works, but here's what I listed:) Students cheating a test, Politicians focusing on style over substance for votes, Company imposing externalized costs like pollution to cut costs, Newswriters going for outrage over quality.</p>"
"<p>Goodhart's Law, as a <strong>causal diagram</strong>:</p>";"<img src='aisffs-CausalGoodhart.png'>"
"<p>The problem with optimization, described mathematically:</p>";"<p>If something has 100 variables, and you set goals on 10 of them, by default, the remaining 90 will be pushed to extreme values.</p>"
"<p>Security Mindset, step one:</p>";"<p>Ask: “What's the worst that could (plausibly) happen?”</p>"
"<p>Security Mindset, step two:</p>";"<p>Fix the problem <em>before</em> it can happen.</p>"
"<p>Name two fields that use Security Mindset:</p>";"<p>(Any of these work:) Engineering elevators, airplanes, bridges, rockets, cyber-security.</p>"
"<p>The problem isn't that advanced AIs won't <em>know</em> what we want...</p>";"<p>... it's that it won't <em>care</em>. (Analogy: reward-chasing politicians/CEOs may fully know the harm they do, yet not care.)</p>"
"<p>What is 'wireheading'?</p>";"<p>When an agent directly hacks its own reward circuits. (An AI hacking itself, or a human directly stimulating their brain's reward circuits.)</p>"
"<p>Why is it hard to program an AI to 'do what I want'?</p>";"<p>'What I want' is <em>still</em> hard to rigorously define well.</p>"
"<p>Why isn't 'what I want' = 'what I consistently choose'?</p>";"<p>We ~all have bad habits: consistently choosing things we don't value.</p>"
"<p>Why isn't 'what I want' = 'what feels rewarding'?</p>";"<p>Wireheading &amp; hard drugs create reward-signals in the brain, but most folks want to avoid them not in spite of that, but <em>because</em> of that.</p>"
"<p>A more better goal than 'do what I want':</p>";"<p>Do what I <em>would have</em> wanted, if I knew the outcomes in advance.</p>"
"<p>When you would <em>want</em> an AI to disobey you:</p>";"<p>If you mistakenly request something that would be bad for your true goals. (e.g. telling Robot to fetch water to put out a grease fire)</p>"
"<p>Game theory is the math of...</p>";"<p>how decision-makers (human or AI) behave</p>"
"<p>Name two fields that use game theory</p>";"<p>(Any 2 of the following work:) AI, economics, evolutionary biology, computer science</p>"
"<p>What's the standard visual tool in game theory, to understand how agents make decisions, in sequence?</p>";"<p><strong>Game Tree!</strong> (example below)</p><img src='aisffs-gametree.png'>"
"<p>A game tree shows 2 things:</p>";"<p>a) The possible decisions that <em>could</em> have been made, and b) Who makes what decisions in what order.</p>"
"<p>With a game tree, how do you predict what <em>actual</em> decisions will be made?</p>";"<p>Backwards induction</p>"
"<p>What we call it when an AI's incentives are 'okay with' you shutting it down or changing its goals:</p>";"<p>Corrigible</p>"
"<p>Name of the 'game' that tells us when AIs have an incentive to prevent users from shutting it down:</p>";"<p>The Off-Switch Game</p>"
"<p>The wireheading problem can be solved with this possibly-worse problem:</p>";"<p>Instrumental Convergence</p>"
"<p>Instead of thinking of a goal-directed AI as 'a human chasing reward', think of it as...</p>";"<p>...a sorting algorithm that 1) Sorts actions by some criteria, then 2) Does the top-ranked action.</p>"
"<p>When someone says an AI 'cares' about X, or its goal is X, or it's rewarded for X... that's just a shorthand for saying:</p>";"<p>An AI <em>sorts &amp; selects actions based on</em> X.</p>"
"<p>Why may an AI not necessarily 'care' about a number labelled REWARD? An analogy:</p>";"<p>(Either analogy works:) You wouldn't care about becoming a sand-grain millionaire, or handwriting extra zeroes on your printed-out bank statement balance.</p>"
"<p>Robot can accurately predict that if it <em>did</em> wirehead, it would only care about the reward counter in its head, and that's why it chooses to <em>not</em> wirehead. <strong>What's an analogy to humans?</strong></p>";"<p>You can realize that <em>if</em> you took a highly-addictive drug, you'd only want that drug, and that's <em>why</em> you avoid that drug. (alternatively: direct brain stimulation, lotus-eaters)</p>"
"<p>Name at least 4 'basic AI drives':</p>";"<p>Any 4 of the following work: Self-preservation, Preventing shutdown, Preventing wireheading, Preventing <em>you</em> from altering its goals, Becoming smarter, Grabbing resources/power, Persuasion, Deception</p>"
"<p>What is 'regression' in statistics?</p>";"<p>Fitting a curve to data. [technically, it's fitting a <em>mathematical function</em> to data.]</p><img src='aisffs-regression.png'>"
"<p>Visualize what a '<em>linear</em> regression' looks like:</p>";"<img src='aisffs-linear.png'>"
"<p>What's a 'model' in science/statistics?</p>";"<p>A simplified mathematical version of a real-world thing. (Like how model trains are small versions of real trains)</p>"
"<p>What is a 'parameter', intuitively?</p>";"<p>A little knob you can twist to adjust a model. (Mathematically, it's just a number)</p>"
"<p>Why is it that when a model has more parameters, what each parameter 'does' becomes harder to interpret?</p>";"<p>Because what a parameter 'does' depends on the others. The more 'others', the harder to say what it 'does'.</p>"
"<p>Why doesn't anybody understand our cutting-edge AIs (yet)?</p>";"<p>Because the more parameters, the harder to interpret, and our frontier AI models have over a Trillion parameters.</p>"
"<p>When someone creates an input to an AI that exploits its fragility, that's called:</p>";"<p>an <strong>adversarial example.</strong></p>"
"<p>When AI fails on new data that's outside the range of the training dataset, that's called:</p>";"<p>an <strong>out-of-distribution error</strong> (or OOD error for short)</p>"
"<p>Training Error is...</p>";"<p>the error a model gets on the data it's trained on.</p>"
"<p>Test Error is...</p>";"<p>the error a model gets on <em>new</em> data it was not trained on. (Like how 'tests' in schools are filled with questions you did not see in class or homework, your 'training data')</p>"
"<p>Underfitting is...</p>";"<p>When a model is too simple, it does badly in training <em>and</em> in the real-world (test).</p>"
"<p>Overfitting is...</p>";"<p>When a model is too complex. It does great in training, but badly in the real-world (test)</p>"
"<p>Visualize a graph of Training &amp; Test Error over model complexity/number of parameters:</p>";"<img src='aisffs-fit_vs_params.png'>"
"<p>When is overfitting likely?</p>";"<p>When the model has <em>more parameters</em> than we have training data points.</p>"
"<p>Why are modern ANNs so prone to overfitting &amp; fragility?</p>";"<p>Because ANNs need a high # of parameters to be useful, so we also need high amounts of data to prevent overfitting, but it's hard to find/make enough unique data.</p>"
"<p>A paradox about modern ANNs, about their # of parameters vs # of training data items</p>";"<p>A lot of modern ANNs have far more parameters than training data items, yet somehow aren't <em>total</em> messes. (This is because of a few techniques to reduce overfitting.)</p>"
"<p>The simple explanation for Algorithmic Bias</p>";"<p>Bias in, bias out. (The training data itself is biased, due to discrimination in the past, or bias in selecting the data.)</p>"
"<p>A deeper explanation for Algorithmic Bias</p>";"<p>Current machine learning acts on <em>correlations</em>, not <em>causations.</em></p>"
"<p>Why would an ML/AI model that doesn't understand causation, be unfairly discriminatory <em>by default?</em></p>";"<p>Because it'll use traits that <em>correlate</em> with one's skill/character/etc, but don't directly <em>cause</em> those.</p>"
"<p>If you find a correlation between A and B, then 3 possible cause-and-effect paths are: (visualize)</p>";"<p>Forwards causation, Backwards causation, Confounded causation. (also possible: Coincidence, Selection/collider bias)</p><img src='aisffs-causal.png'>"
"<p>Common saying: 'Correlation is not evidence of causation'. Technical nitpick:</p>";"<p>Correlation <em>is</em> evidence of causation, in the mathematical sense of 'evidence'. However, correlation alone does not tell you <em>what kind(s)</em> of causation happened.</p>"
"<p>An example of creepy subtle correlations that modern AIs can pick up on</p>";"<p>(Any of these answers work:) AI can predict your gender &amp; ethnicity just from your writing style, or predict your sexual orientation &amp; politics just from your face(!)</p>"
"<p>Example of Goal Mis-generalization</p>";"<p>(Either works:) Robot mis-learns 'clean' as 'wash', CoinRun AI learns to go to end, instead of go to coin.</p>"
"<p>Goal Mis-generalization (GMG) is when an AI does what we want in ___, but not in ___</p>";"<p>in training, but not in real-world / deployment / test</p>"
"<p>You can still get goal mis-generalization even with perfect...</p>";"<p>...perfect goal <em>specification!</em> (What you reward the AI for doing ≠ What goal the AI learns to optimize for.)</p>"
"<p>Two kinds of robustness failures (relevant to Goal Mis-generalization)</p>";"<p>Capabilities fail to generalize, Goals fail to generalize.</p>"
"<p>Why can it be <em>worse</em> for an AI to have broken goals, but intact capabilities?</p>";"<p>It can now <em>skillfully</em> execute on bad goals!</p>"
"<p>Fundamentally, why does goal mis-generalization happen?</p>";"<p>Because current ML/AI systems only do correlation, not causation. (If two goals are <em>merely correlated</em> in the training data, like get-end and get-coin, an AI could learn the wrong one.)</p>"
"<p>Visualize the causal diagram behind the correlation-causation problem, that leads to goal mis-generalization:</p>";"<img src='aisffs-CausalGMG.png'>"
"<p>Why don't Good Ol' Fashioned AIs have goal mis-generalization? [2 reasons]</p>";"<ol>
<li>They <em>can't</em> mis-learn the goal, since you give it to them directly, and 2) They usually <em>can</em> reason about cause-and-effect.</li>
</ol>"
"<p>An example of goal mis-generalization in humans?</p>";"<p>Bad habits that used to be adaptive in our 'training data' (childhood).</p>"
"<p>Example of a human that shows that Smart ≠ Good.</p>";"<p>Wernher von Braun (~pronounced 'Brown'), rocket scientist who got us to the moon &amp; literal Nazi.</p>"
"<p>If 'regular' ethics (applied ethics) asks, 'what should I do in this scenario?' <strong>Meta-ethics</strong> asks,</p>";"<p>'What is the nature of moral truth, anyway? (e.g. Is it universal/objective, like math &amp; physics?)'</p>"
"<p>Even if moral truth objectively existed, why might an advanced general AI still not be moral?</p>";"<p>(Any of the following:) Without consciousness/a soul, it may not even <em>perceive</em> moral truths. Or moral rules may not even <em>apply</em> to non-conscious agents.</p>"
"<p>What are the Big Three approaches to grounding morality in secular rationality?</p>";"<p>Virtue Ethics, Deontology, Utilitarianism</p>"
"<p>What's the 'social contract' theory of ethics say?</p>";"<p>It says morality doesn't objectively exist, but it's useful to pretend it does, so we can coordinate on a 'contract', to attain our own interests.</p>"
"<p>Why may an advanced AI not be beholden to a social contract?</p>";"<p>If it gets <em>too</em> powerful, we can't enforce a contract against it.</p>"